# -*- coding: utf-8 -*-
import io, re, unicodedata, json
from typing import List, Dict, Any, Tuple
import pandas as pd
import streamlit as st

# Lectura de PDF/DOCX
from pdfminer.high_level import extract_text as pdf_extract_text
from docx import Document

# Google Drive
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CONFIG STREAMLIT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(page_title="Extractor de √ìrdenes del D√≠a", page_icon="üóÇÔ∏è", layout="wide")
st.title("üóÇÔ∏è Extractor de √ìrdenes del D√≠a ‚Üí Planilla est√°ndar + Drive (Looker-ready)")

DEFAULT_FOLDER_ID = "REEMPLAZAR_CON_TU_CARPETA"  # fallback si no est√° en secrets
CSV_NAME  = "OrdenDelDia_Consejo.csv"
XLSX_NAME = "OrdenDelDia_Consejo.xlsx"
SHEET_NAME = "OrdenDelDia"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UTILIDADES GENERALES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def norm(s: str) -> str:
    if not isinstance(s, str): return ""
    s = unicodedata.normalize("NFKC", s).replace("\xa0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{2,}", "\n", s)
    return s.strip()

def extract_text_any(uploaded) -> str:
    name = uploaded.name.lower()
    if name.endswith(".pdf"):
        return norm(pdf_extract_text(uploaded))
    if name.endswith(".docx"):
        doc = Document(uploaded)
        return norm("\n".join(p.text for p in doc.paragraphs))
    return ""

def find_date_header(text: str) -> Tuple[str, str]:
    head = text[:1500]
    m = re.search(r"\b(\d{1,2})[\/\-\._](\d{1,2})[\/\-\._](\d{2,4})\b", head)
    if m:
        d, mth, y = m.groups()
        y = "20"+y if len(y) == 2 else y
        y = int(y)
        d = f"{int(d):02d}"; mth = f"{int(mth):02d}"
        return str(y), f"{d}/{mth}/{y}"
    m2 = re.search(r"a\s+los\s+\d+\s+d[i√≠]as.*?mes\s+de\s+[a-z√°√©√≠√≥√∫]+.*?dos\s+mil\s+([a-z√°√©√≠√≥√∫]+)", head, re.I|re.S)
    if m2:
        mapa = {"veinte":2020,"veintiuno":2021,"veintidos":2022,"veintitres":2023,"veinticuatro":2024,"veinticinco":2025}
        y = unicodedata.normalize("NFKD", m2.group(1)).lower()
        y = y.replace("ÃÅ","").replace("√°","a").replace("√©","e").replace("√≠","i").replace("√≥","o").replace("√∫","u")
        if y in mapa:
            for ln in head.split("\n"):
                if len(ln.strip()) > 12:
                    return str(mapa[y]), ln.strip()
    m3 = re.search(r"\b(20\d{2})\b", head)
    if m3:
        return m3.group(1), ""
    return "", ""

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ESQUEMA FIJO DE COLUMNAS (Looker)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
FIXED_COLUMNS = [
    "a√±o","fecha",
    "proyectos de investigaci√≥n","Nombre del proyecto de investigaci√≥n","Director del Proyecto","Integrantes del equipo de investigaci√≥n","Unidad acad√©mica de procedencia del proyecto",
    "Informe de avance","Nombre del proyecto de investigaci√≥n del Informe de avance","Director del Proyecto del Informe de avance","Integrantes del equipo de investigaci√≥n del Informe de avance","Unidad acad√©mica de procedencia del proyecto del Informe de avance",
    "Informe Final","Nombre del proyecto de investigaci√≥n del Informe Final","Director del Proyecto del Informe Final","Integrantes del equipo de investigaci√≥n del Informe Final","Unidad acad√©mica de procedencia del proyecto del Informe Final",
    "Proyectos de investigaci√≥n de c√°tedra","Nombre del proyecto de investigaci√≥n c√°tedra","Director del Proyecto del Informe de c√°tedra","Integrantes del equipo de investigaci√≥n del proyecto de c√°tedra","Unidad acad√©mica de procedencia del proyecto de c√°tedra",
    "Publicaci√≥n","Tipo de publicaci√≥n (revista cient√≠fica, libro, presentaci√≥n a congreso, p√≥ster, revista Cuadernos, manual)","Docente o investigador incluida en la publicaci√≥n","Unidad acad√©mica (Publicaci√≥n)",
    "Categorizaci√≥n de docentes","Nombre del docente categorizado como investigador","Categor√≠a alcanzada por el docente como docente investigador","Unidad acad√©mica (Categorizaci√≥n)",
    "Becario de beca cofinanciada doctoral","Nombre del becario doctoral","Becario de beca cofinanciada postdoctoral","Nombre del becario postdoctoral",
    "OTROS TEMAS"
]
def empty_row(base=None):
    row = {c:"" for c in FIXED_COLUMNS}
    if base: row.update({k:v for k,v in base.items() if k in row})
    return row

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PARSER ROBUSTO (exactitud de t√≠tulos)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Encabezados de secci√≥n
SECTION_HEADERS = {
    "proyectos": re.compile(r"^(presentaci[o√≥]n\s+de\s+proyectos?|proyectos?\s+(de\s+)?investigaci[o√≥]n)\b", re.I),
    "final":     re.compile(r"^informes?\s+final(?:es)?\b", re.I),
    "avance":    re.compile(r"^informes?\s+de\s+avance\b", re.I),
    "catedra":   re.compile(r"(proyectos?\s+(de\s+)?c[a√°]tedra|proyectos?\s+cuadernos)", re.I),
    "publica":   re.compile(r"^publicaci[o√≥]n(?:es)?\b", re.I),
    "categ":     re.compile(r"^categorizaci[o√≥]n", re.I),
    "beca":      re.compile(r"^becari[oa]s?\b", re.I),
}
# R√≥tulos
DIRECTOR_LABELS = re.compile(r"\b(Director(?:a)?|Co[- ]?director(?:a)?)\b\s*:", re.I)
TEAM_LABELS     = re.compile(r"\b(Equipo(?:\s+de\s+(Trabajo|Investigaci[o√≥]n))?|Integrantes|Investigadores|Docentes|Estudiantes)\b\s*:", re.I)
UNIT_LABELS     = re.compile(r"\b(Facultad|Escuela|Instituto|Vicerrectorado)\b", re.I)

# L√≠neas que NUNCA son t√≠tulos (ruido t√≠pico)
NO_TITLE_PREFIX = re.compile(r"^(L[i√≠]neas? de investigaci[o√≥]n|Enfoque[s]?:|Programa|Jornadas|PEI|Plan|Comisi[o√≥]n|Convocatoria)\b", re.I)

def split_lines(text: str) -> List[str]:
    # separa por l√≠neas y bullets, limpia vi√±etas y guiones
    text = re.sub(r"[\u2022\u2023\u25CF\u25CB\u25A0‚Ä¢‚ñ∫‚ñ™‚ñ´]+", "\n", text)
    lines = [norm(ln.strip(" \t-‚Äî‚Äì‚Ä¢")) for ln in text.split("\n")]
    return [ln for ln in lines if ln]

def is_section_header(ln: str) -> str:
    for key, rx in SECTION_HEADERS.items():
        if rx.search(ln): return key
    return ""

def is_faculty_line(ln: str) -> bool:
    return bool(re.match(r"^(Facultad|Escuela|Instituto|Vicerrectorado)\b", ln, re.I))

def looks_title_line(ln: str) -> bool:
    if not ln or len(ln) < 6: return False
    if NO_TITLE_PREFIX.search(ln): return False
    if DIRECTOR_LABELS.search(ln) or TEAM_LABELS.search(ln) or UNIT_LABELS.search(ln): return False
    # Comillas = muy probable t√≠tulo
    if re.search(r"[¬´‚Äú\"'].*[¬ª‚Äù\"']", ln): return True
    # Heur√≠stica de may√∫sculas / Title Case
    alpha = sum(ch.isalpha() for ch in ln)
    caps  = sum(ch.isupper() for ch in ln if ch.isalpha())
    return alpha >= 6 and (ln.istitle() or (alpha > 0 and caps/alpha >= 0.40))

def extract_after(label_rx: re.Pattern, chunk: List[str]) -> str:
    for ln in chunk:
        m = re.search(label_rx, ln)
        if m:
            return norm(re.sub(label_rx, "", ln)).strip(" .,:;‚Äì-\"'¬´¬ª‚Äú‚Äù")
    return ""

def extract_unit(chunk: List[str]) -> str:
    # 1) l√≠nea que empieza con Facultad/Escuela/Instituto/Vicerrectorado
    for ln in chunk:
        if is_faculty_line(ln): return ln
    # 2) cualquier l√≠nea que contenga esas palabras
    for ln in chunk:
        if UNIT_LABELS.search(ln): return ln
    return ""

def cut_before_director(chunk: List[str]) -> List[str]:
    # Cortar el bloque antes de la primera l√≠nea "Director:"
    out = []
    for ln in chunk:
        if DIRECTOR_LABELS.search(ln): break
        out.append(ln)
    return out

def first_title_from(chunk: List[str]) -> str:
    pre = cut_before_director(chunk)
    for ln in pre:
        if looks_title_line(ln):
            return ln.strip(" .,:;‚Äì-\"'¬´¬ª‚Äú‚Äù")
    for ln in pre:
        if ln and not NO_TITLE_PREFIX.search(ln):
            return ln.strip(" .,:;‚Äì-\"'¬´¬ª‚Äú‚Äù")
    return ""

def parse_items_by_section(lines: List[str], base_meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    section = ""
    buf: List[str] = []
    current_unit = ""

    def flush():
        nonlocal buf, section, current_unit
        if not buf: return
        chunk = [ln for ln in buf if ln]
        # descartar bloques completos que son meta/ruido institucional
        joined = " ".join(chunk)
        if NO_TITLE_PREFIX.match(chunk[0]) and not any(DIRECTOR_LABELS.search(x) for x in chunk):
            buf.clear(); return

        unit_here = extract_unit(chunk) or current_unit

        def make_row():
            r = empty_row(base_meta)
            r["Unidad acad√©mica de procedencia del proyecto"] = unit_here
            return r

        if section == "proyectos":
            r = make_row()
            r["proyectos de investigaci√≥n"] = "S√≠"
            r["Nombre del proyecto de investigaci√≥n"] = first_title_from(chunk)
            r["Director del Proyecto"] = extract_after(DIRECTOR_LABELS, chunk)
            r["Integrantes del equipo de investigaci√≥n"] = extract_after(TEAM_LABELS, chunk)
            r["Unidad acad√©mica de procedencia del proyecto"] = unit_here
            if r["Nombre del proyecto de investigaci√≥n"]: rows.append(r)

        elif section == "final":
            r = make_row()
            r["Informe Final"] = "S√≠"
            r["Nombre del proyecto de investigaci√≥n del Informe Final"] = first_title_from(chunk)
            r["Director del Proyecto del Informe Final"] = extract_after(DIRECTOR_LABELS, chunk)
            r["Integrantes del equipo de investigaci√≥n del Informe Final"] = extract_after(TEAM_LABELS, chunk)
            r["Unidad acad√©mica de procedencia del proyecto del Informe Final"] = unit_here
            if r["Nombre del proyecto de investigaci√≥n del Informe Final"]: rows.append(r)

        elif section == "avance":
            r = make_row()
            r["Informe de avance"] = "S√≠"
            r["Nombre del proyecto de investigaci√≥n del Informe de avance"] = first_title_from(chunk)
            r["Director del Proyecto del Informe de avance"] = extract_after(DIRECTOR_LABELS, chunk)
            r["Integrantes del equipo de investigaci√≥n del Informe de avance"] = extract_after(TEAM_LABELS, chunk)
            r["Unidad acad√©mica de procedencia del proyecto del Informe de avance"] = unit_here
            if r["Nombre del proyecto de investigaci√≥n del Informe de avance"]: rows.append(r)

        elif section == "catedra":
            r = make_row()
            r["Proyectos de investigaci√≥n de c√°tedra"] = "S√≠"
            r["Nombre del proyecto de investigaci√≥n c√°tedra"] = first_title_from(chunk)
            r["Director del Proyecto del Informe de c√°tedra"] = extract_after(DIRECTOR_LABELS, chunk)
            r["Integrantes del equipo de investigaci√≥n del proyecto de c√°tedra"] = extract_after(TEAM_LABELS, chunk)
            r["Unidad acad√©mica de procedencia del proyecto de c√°tedra"] = unit_here
            if r["Nombre del proyecto de investigaci√≥n c√°tedra"]: rows.append(r)

        elif section == "publica":
            r = make_row()
            r["Publicaci√≥n"] = "S√≠"
            tx = " ".join(chunk)
            tipo = ""
            if re.search(r"\brevista\b", tx, re.I): tipo = "revista cient√≠fica"
            elif re.search(r"\blibro\b", tx, re.I): tipo = "libro"
            elif re.search(r"\b(congreso|ponencia|presentaci[o√≥]n)\b", tx, re.I): tipo = "presentaci√≥n a congreso"
            elif re.search(r"p[o√≥]ster|poster", tx, re.I): tipo = "p√≥ster"
            elif re.search(r"\bcuadernos\b", tx, re.I): tipo = "revista Cuadernos"
            elif re.search(r"\bmanual\b", tx, re.I): tipo = "manual"
            r["Tipo de publicaci√≥n (revista cient√≠fica, libro, presentaci√≥n a congreso, p√≥ster, revista Cuadernos, manual)"] = tipo
            r["Docente o investigador incluida en la publicaci√≥n"] = extract_after(re.compile(r"(Autor(?:es)?|Docente|Investigador(?:es)?)\s*:", re.I), chunk)
            r["Unidad acad√©mica (Publicaci√≥n)"] = unit_here
            rows.append(r)

        elif section == "categ":
            r = make_row()
            r["Categorizaci√≥n de docentes"] = "S√≠"
            joined = " | ".join(chunk)
            mcat = re.search(r"(Categor[i√≠]a\s*[:\-]?\s*[IVX]+|Investigador(?:\s+\w+){0,3})", joined, re.I)
            r["Categor√≠a alcanzada por el docente como docente investigador"] = mcat.group(0) if mcat else ""
            cand = next((ln for ln in chunk if re.search(r"^[A-Z√Å√â√ç√ì√ö√ë][A-Z√Å√â√ç√ì√ö√ë ]+,\s*[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+", ln)), "")
            r["Nombre del docente categorizado como investigador"] = cand
            r["Unidad acad√©mica (Categorizaci√≥n)"] = unit_here
            rows.append(r)

        elif section == "beca":
            r = make_row()
            if re.search(r"postdoctoral", " ".join(chunk), re.I):
                r["Becario de beca cofinanciada postdoctoral"] = "S√≠"
                r["Nombre del becario postdoctoral"] = first_title_from(chunk) or extract_after(re.compile(r"(Becari[oa]|Nombre)\s*:", re.I), chunk)
            else:
                r["Becario de beca cofinanciada doctoral"] = "S√≠"
                r["Nombre del becario doctoral"] = first_title_from(chunk) or extract_after(re.compile(r"(Becari[oa]|Nombre)\s*:", re.I), chunk)
            rows.append(r)

        else:
            r = make_row()
            r["OTROS TEMAS"] = " ".join(chunk)
            rows.append(r)

        buf.clear()
        current_unit = unit_here

    # Recorrido
    for ln in lines:
        sec = is_section_header(ln)
        if sec:
            flush(); section = sec; continue
        if is_faculty_line(ln):
            # Un nuevo bloque de Unidad delimita √çTEMS; flush previo
            flush(); current_unit = ln; continue
        buf.append(ln)
    flush()
    return rows

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GOOGLE DRIVE (reemplazo por nombre o por ID)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_creds(scopes):
    sa = st.secrets.get("gcp_service_account")
    if not sa: return None
    if isinstance(sa, dict):
        return Credentials.from_service_account_info(sa, scopes=scopes)
    try:
        return Credentials.from_service_account_info(json.loads(sa), scopes=scopes)
    except Exception:
        return None

def drive_client(creds):
    return build("drive", "v3", credentials=creds)

def drive_find_file(drive, name: str, folder_id: str) -> str:
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = drive.files().list(q=q, fields="files(id,name)", pageSize=1).execute()
    files = res.get("files", [])
    return files[0]["id"] if files else ""

def drive_upload_replace(drive, folder_id: str, name: str, data: bytes, mime: str, file_id_hint: str = ""):
    media = MediaIoBaseUpload(io.BytesIO(data), mimetype=mime, resumable=False)
    if file_id_hint:
        drive.files().update(fileId=file_id_hint, media_body=media).execute()
        return file_id_hint
    fid = drive_find_file(drive, name, folder_id)
    if fid:
        drive.files().update(fileId=fid, media_body=media).execute()
        return fid
    meta = {"name": name, "parents": [folder_id]}
    f = drive.files().create(body=meta, media_body=media, fields="id").execute()
    return f["id"]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UI
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.subheader("1) Sub√≠ el/los √ìrdenes del D√≠a (PDF o DOCX)")
uploads = st.file_uploader("üìÇ Archivos", type=["pdf","docx"], accept_multiple_files=True)

if not uploads:
    st.info("Sub√≠ al menos un archivo para continuar."); st.stop()

all_rows = []
for up in uploads:
    raw = extract_text_any(up)
    if not raw:
        st.warning(f"No se pudo leer: {up.name}")
        continue
    year, date_str = find_date_header(raw)
    base = {"a√±o": year, "fecha": date_str}
    lines = split_lines(raw)
    rows = parse_items_by_section(lines, base)
    if not rows:
        r = empty_row(base); r["OTROS TEMAS"] = raw[:1500] + ("‚Ä¶" if len(raw) > 1500 else ""); rows = [r]
    all_rows.extend(rows)

if not all_rows:
    st.error("No se detectaron √≠tems en los √ìrdenes del D√≠a cargados."); st.stop()

df = pd.DataFrame(all_rows)
for col in FIXED_COLUMNS:
    if col not in df.columns: df[col] = ""
df = df[FIXED_COLUMNS]

st.success("‚úÖ √ìrdenes del D√≠a procesados.")
st.dataframe(df, use_container_width=True)

# Descargas
st.subheader("2) Descargar planillas")
csv_bytes = df.to_csv(index=False).encode("utf-8")
st.download_button("üìó CSV (OrdenDelDia_Consejo.csv)", data=csv_bytes, file_name=CSV_NAME, mime="text/csv")

def to_xlsx_bytes(df0: pd.DataFrame) -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as w:
        df0.to_excel(w, index=False, sheet_name=SHEET_NAME)
    buf.seek(0); return buf

xlsx_buf = to_xlsx_bytes(df)
st.download_button("üìò Excel (OrdenDelDia_Consejo.xlsx)", data=xlsx_buf, file_name=XLSX_NAME,
                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

# Drive
st.subheader("3) Subir/Reemplazar en Google Drive (para Looker Studio)")
folder_id = st.secrets.get("drive_folder_id", DEFAULT_FOLDER_ID)
creds = get_creds(["https://www.googleapis.com/auth/drive"])
csv_id_secret  = st.secrets.get("drive_csv_file_id", "")
xlsx_id_secret = st.secrets.get("drive_xlsx_file_id", "")

if not creds:
    st.caption("‚ÑπÔ∏è Configur√° `gcp_service_account` en Secrets para habilitar Drive.")
else:
    if st.button("üöÄ Subir/Reemplazar CSV y Excel en Drive"):
        try:
            drv = drive_client(creds)
            csv_id  = drive_upload_replace(drv, folder_id, CSV_NAME, csv_bytes, "text/csv", file_id_hint=csv_id_secret)
            xlsx_id = drive_upload_replace(drv, folder_id, XLSX_NAME, xlsx_buf.getvalue(),
                                           "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                           file_id_hint=xlsx_id_secret)
            st.success("‚úÖ Archivos actualizados en Drive.")
            st.caption(f"CSV id: {csv_id} ¬∑ XLSX id: {xlsx_id}")
            st.info("Looker Studio se actualiza solo al mantener los mismos IDs.")
        except Exception as e:
            st.error("‚ùå Error subiendo a Drive.")
            st.exception(e)
