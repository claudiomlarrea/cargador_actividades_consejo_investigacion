# -*- coding: utf-8 -*-
import io, re, unicodedata, json
from typing import List, Dict, Any, Tuple
import pandas as pd
import streamlit as st

# Lectura de PDF/DOCX
from pdfminer.high_level import extract_text as pdf_extract_text
from docx import Document

# Google Drive
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG STREAMLIT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Extractor de Ã“rdenes del DÃ­a", page_icon="ğŸ—‚ï¸", layout="wide")
st.title("ğŸ—‚ï¸ Extractor de Ã“rdenes del DÃ­a â†’ Planilla estÃ¡ndar + Drive (Looker-ready)")

DEFAULT_FOLDER_ID = "REEMPLAZAR_CON_TU_CARPETA"  # fallback si no estÃ¡ en secrets
CSV_NAME  = "OrdenDelDia_Consejo.csv"
XLSX_NAME = "OrdenDelDia_Consejo.xlsx"
SHEET_NAME = "OrdenDelDia"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UTILIDADES GENERALES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def norm(s: str) -> str:
    if not isinstance(s, str): return ""
    s = unicodedata.normalize("NFKC", s).replace("\xa0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{2,}", "\n", s)
    return s.strip()

def extract_text_any(uploaded) -> str:
    """Devuelve texto plano de PDF o DOCX."""
    name = uploaded.name.lower()
    if name.endswith(".pdf"):
        return norm(pdf_extract_text(uploaded))
    if name.endswith(".docx"):
        doc = Document(uploaded)
        return norm("\n".join(p.text for p in doc.paragraphs))
    return ""

def find_date_header(text: str) -> Tuple[str, str]:
    """
    Intenta detectar fecha/aÃ±o de la reuniÃ³n en el encabezado o nombre.
    Formatos esperados: 20/02/2025, 21-08-25, 18_04_24, 23/10/2025, etc.
    """
    head = text[:1200]
    # dd[/-_]mm[/-_]yyyy | dd[/-_]mm[/-_]yy
    m = re.search(r"\b(\d{1,2})[\/\-\._](\d{1,2})[\/\-\._](\d{2,4})\b", head)
    if m:
        d, mth, y = m.groups()
        y = "20"+y if len(y) == 2 else y
        y = int(y)
        d = f"{int(d):02d}"; mth = f"{int(mth):02d}"
        return str(y), f"{d}/{mth}/{y}"
    # â€œa los â€¦ dÃ­as del mes de â€¦ de dos mil â€¦â€
    m2 = re.search(r"a\s+los\s+\d+\s+d[iÃ­]as.*?mes\s+de\s+[a-zÃ¡Ã©Ã­Ã³Ãº]+.*?dos\s+mil\s+([a-zÃ¡Ã©Ã­Ã³Ãº]+)", head, re.I|re.S)
    if m2:
        mapa = {
            "veinte":2020,"veintiuno":2021,"veintidos":2022,"veintitres":2023,"veinticuatro":2024,
            "veinticinco":2025,"veintiseis":2026,"veintisiete":2027,"veintiocho":2028,"veintinueve":2029,"treinta":2030
        }
        y = mapa.get(unicodedata.normalize("NFKD", m2.group(1)).replace("Ì","").lower())
        if y:
            # tomar primera lÃ­nea larga como "fecha textual"
            for ln in head.split("\n"):
                if len(ln.strip()) > 12:
                    return str(y), ln.strip()
    # fallback: primer aÃ±o 20xx
    m3 = re.search(r"\b(20\d{2})\b", head)
    if m3:
        return m3.group(1), ""
    return "", ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ESQUEMA FIJO DE COLUMNAS (para Looker Studio)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FIXED_COLUMNS = [
    "aÃ±o",
    "fecha",

    "proyectos de investigaciÃ³n",
    "Nombre del proyecto de investigaciÃ³n",
    "Director del Proyecto",
    "Integrantes del equipo de investigaciÃ³n",
    "Unidad acadÃ©mica de procedencia del proyecto",

    "Informe de avance",
    "Nombre del proyecto de investigaciÃ³n del Informe de avance",
    "Director del Proyecto del Informe de avance",
    "Integrantes del equipo de investigaciÃ³n del Informe de avance",
    "Unidad acadÃ©mica de procedencia del proyecto del Informe de avance",

    "Informe Final",
    "Nombre del proyecto de investigaciÃ³n del Informe Final",
    "Director del Proyecto del Informe Final",
    "Integrantes del equipo de investigaciÃ³n del Informe Final",
    "Unidad acadÃ©mica de procedencia del proyecto del Informe Final",

    "Proyectos de investigaciÃ³n de cÃ¡tedra",
    "Nombre del proyecto de investigaciÃ³n cÃ¡tedra",
    "Director del Proyecto del Informe de cÃ¡tedra",
    "Integrantes del equipo de investigaciÃ³n del proyecto de cÃ¡tedra",
    "Unidad acadÃ©mica de procedencia del proyecto de cÃ¡tedra",

    "PublicaciÃ³n",
    "Tipo de publicaciÃ³n (revista cientÃ­fica, libro, presentaciÃ³n a congreso, pÃ³ster, revista Cuadernos, manual)",
    "Docente o investigador incluida en la publicaciÃ³n",
    "Unidad acadÃ©mica (PublicaciÃ³n)",  # â† desambiguado para mantener unicidad de columnas

    "CategorizaciÃ³n de docentes",
    "Nombre del docente categorizado como investigador",
    "CategorÃ­a alcanzada por el docente como docente investigador",
    "Unidad acadÃ©mica (CategorizaciÃ³n)",  # â† desambiguado

    # Becarios: unificamos como dos pares (doctoral/postdoctoral)
    "Becario de beca cofinanciada doctoral",
    "Nombre del becario doctoral",
    "Becario de beca cofinanciada postdoctoral",
    "Nombre del becario postdoctoral",

    "OTROS TEMAS"  # todo lo que no encaje arriba
]

def empty_row(base: Dict[str, Any]=None) -> Dict[str, Any]:
    row = {col: "" for col in FIXED_COLUMNS}
    if base:
        row.update({k:v for k,v in base.items() if k in row})
    return row

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PARSER DE SECCIONES (Ã“RDENES DEL DÃA)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION_MAP = {
    "proyectos": re.compile(r"^(proyectos? (de )?investigaci[oÃ³]n|presentaci[oÃ³]n de proyectos?)\b", re.I),
    "avance":    re.compile(r"^informes? de avance\b", re.I),
    "final":     re.compile(r"^informes? finales?\b", re.I),
    "catedra":   re.compile(r"(proyectos? (de )?c[aÃ¡]tedra|proyectos? cuadernos)", re.I),
    "publica":   re.compile(r"^publicaci[oÃ³]n|^publicaciones\b", re.I),
    "categ":     re.compile(r"^categorizaci[oÃ³]n", re.I),
    "beca":      re.compile(r"^becari[oa]s?", re.I),
}

def split_lines(text: str) -> List[str]:
    lines = [ln.strip(" -â€¢\t") for ln in text.split("\n")]
    return [ln for ln in lines if ln]

def current_section_of(line: str) -> str:
    l = line.strip()
    for key, rx in SECTION_MAP.items():
        if rx.search(l):
            return key
    return ""

def extract_name_after(label: str, txt: str) -> str:
    m = re.search(label + r"\s*:\s*(.+)", txt, re.I)
    return norm(m.group(1)) if m else ""

def parse_people_list(s: str) -> str:
    s = re.sub(r"\s*[â€“â€”-]\s*", " â€“ ", s)
    s = s.replace(" ,", ",")
    return norm(s)

def parse_unit(s: str) -> str:
    m = re.search(r"(Facultad|Escuela|Instituto|Vicerrectorado)[^\n]*", s, re.I)
    return norm(m.group(0)) if m else ""

def looks_title_line(s: str) -> bool:
    # heurÃ­stica para lÃ­neas de TÃTULO
    if len(s) < 6: return False
    if re.search(r"(Director|Directora|Integrantes|Equipo|Codirector|Unidad)", s, re.I): return False
    cap = sum(1 for c in s if c.isupper())
    alpha = sum(1 for c in s if c.isalpha())
    return (alpha > 0 and (cap/alpha) > 0.4) or s.istitle() or "â€œ" in s or '"' in s

def parse_items_by_section(lines: List[str], base_meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Crea filas WIDE conforme a FIXED_COLUMNS.
    Una fila por Ã­tem. Campos no aplicables quedan vacÃ­os.
    """
    rows: List[Dict[str, Any]] = []
    sec = ""
    buf: List[str] = []

    def flush_buffer(section: str, buffer: List[str]):
        if not buffer: return
        chunk = "\n".join(buffer)
        row_base = empty_row(base_meta)
        # Ruteo por secciÃ³n
        if section == "proyectos":
            row_base["proyectos de investigaciÃ³n"] = "SÃ­"
            # TÃ­tulo
            t = extract_name_after(r"(Denominaci[oÃ³]n|T[iÃ­]tulo|Proyecto)", chunk) or \
                next((ln for ln in buffer if looks_title_line(ln)), "")
            row_base["Nombre del proyecto de investigaciÃ³n"] = t.strip("â€œâ€\"' ")
            # Director / Integrantes / Unidad
            row_base["Director del Proyecto"] = extract_name_after(r"Director(?:a)?", chunk)
            integ = extract_name_after(r"(Integrantes|Equipo)", chunk)
            row_base["Integrantes del equipo de investigaciÃ³n"] = parse_people_list(integ)
            row_base["Unidad acadÃ©mica de procedencia del proyecto"] = parse_unit(chunk)
            rows.append(row_base)

        elif section == "avance":
            row_base["Informe de avance"] = "SÃ­"
            t = extract_name_after(r"(Denominaci[oÃ³]n|T[iÃ­]tulo|Proyecto)", chunk) or \
                next((ln for ln in buffer if looks_title_line(ln)), "")
            row_base["Nombre del proyecto de investigaciÃ³n del Informe de avance"] = t.strip("â€œâ€\"' ")
            row_base["Director del Proyecto del Informe de avance"] = extract_name_after(r"Director(?:a)?", chunk)
            row_base["Integrantes del equipo de investigaciÃ³n del Informe de avance"] = parse_people_list(
                extract_name_after(r"(Integrantes|Equipo)", chunk)
            )
            row_base["Unidad acadÃ©mica de procedencia del proyecto del Informe de avance"] = parse_unit(chunk)
            rows.append(row_base)

        elif section == "final":
            row_base["Informe Final"] = "SÃ­"
            t = extract_name_after(r"(Denominaci[oÃ³]n|T[iÃ­]tulo|Proyecto)", chunk) or \
                next((ln for ln in buffer if looks_title_line(ln)), "")
            row_base["Nombre del proyecto de investigaciÃ³n del Informe Final"] = t.strip("â€œâ€\"' ")
            row_base["Director del Proyecto del Informe Final"] = extract_name_after(r"Director(?:a)?", chunk)
            row_base["Integrantes del equipo de investigaciÃ³n del Informe Final"] = parse_people_list(
                extract_name_after(r"(Integrantes|Equipo)", chunk)
            )
            row_base["Unidad acadÃ©mica de procedencia del proyecto del Informe Final"] = parse_unit(chunk)
            rows.append(row_base)

        elif section == "catedra":
            row_base["Proyectos de investigaciÃ³n de cÃ¡tedra"] = "SÃ­"
            t = extract_name_after(r"(Denominaci[oÃ³]n|T[iÃ­]tulo|Proyecto|Asignatura)", chunk) or \
                next((ln for ln in buffer if looks_title_line(ln)), "")
            row_base["Nombre del proyecto de investigaciÃ³n cÃ¡tedra"] = t.strip("â€œâ€\"' ")
            row_base["Director del Proyecto del Informe de cÃ¡tedra"] = extract_name_after(r"Director(?:a)?", chunk)
            row_base["Integrantes del equipo de investigaciÃ³n del proyecto de cÃ¡tedra"] = parse_people_list(
                extract_name_after(r"(Integrantes|Equipo|Docentes)", chunk)
            )
            row_base["Unidad acadÃ©mica de procedencia del proyecto de cÃ¡tedra"] = parse_unit(chunk)
            rows.append(row_base)

        elif section == "publica":
            row_base["PublicaciÃ³n"] = "SÃ­"
            # Tipo
            tipo = ""
            for k in ["revista", "libro", "congreso", "pÃ³ster", "poster", "cuadernos", "manual"]:
                if re.search(k, chunk, re.I):
                    mapa = {
                        "revista": "revista cientÃ­fica", "libro": "libro",
                        "congreso":"presentaciÃ³n a congreso", "pÃ³ster":"pÃ³ster", "poster":"pÃ³ster",
                        "cuadernos":"revista Cuadernos", "manual":"manual"
                    }
                    tipo = mapa[k]; break
            row_base["Tipo de publicaciÃ³n (revista cientÃ­fica, libro, presentaciÃ³n a congreso, pÃ³ster, revista Cuadernos, manual)"] = tipo
            # Autor y UA
            row_base["Docente o investigador incluida en la publicaciÃ³n"] = extract_name_after(r"(Autor(?:es)?|Docente|Investigador)", chunk)
            row_base["Unidad acadÃ©mica (PublicaciÃ³n)"] = parse_unit(chunk)
            rows.append(row_base)

        elif section == "categ":
            row_base["CategorizaciÃ³n de docentes"] = "SÃ­"
            row_base["Nombre del docente categorizado como investigador"] = extract_name_after(r"(Docente|Nombre)", chunk) or \
                next((ln for ln in buffer if re.search(r"^[A-ZÃÃ‰ÃÃ“ÃšÃ‘][A-Za-zÃÃ‰ÃÃ“ÃšÃ‘ ]+,", ln)), "")
            row_base["CategorÃ­a alcanzada por el docente como docente investigador"] = extract_name_after(r"(Categor[iÃ­]a|Tipo)", chunk)
            row_base["Unidad acadÃ©mica (CategorizaciÃ³n)"] = parse_unit(chunk)
            rows.append(row_base)

        elif section == "beca":
            # Detectar doctoral/postdoctoral
            if re.search(r"postdoctoral", chunk, re.I):
                row_base["Becario de beca cofinanciada postdoctoral"] = "SÃ­"
                row_base["Nombre del becario postdoctoral"] = extract_name_after(r"(Becari[oa]|Nombre)", chunk) or \
                    next((ln for ln in buffer if re.search(r"^[A-ZÃÃ‰ÃÃ“ÃšÃ‘][A-Za-zÃÃ‰ÃÃ“ÃšÃ‘ ]+$", ln)), "")
            else:
                row_base["Becario de beca cofinanciada doctoral"] = "SÃ­"
                row_base["Nombre del becario doctoral"] = extract_name_after(r"(Becari[oa]|Nombre)", chunk) or \
                    next((ln for ln in buffer if re.search(r"^[A-ZÃÃ‰ÃÃ“ÃšÃ‘][A-Za-zÃÃ‰ÃÃ“ÃšÃ‘ ]+$", ln)), "")
            rows.append(row_base)

        else:
            # OTROS TEMAS
            row_base["OTROS TEMAS"] = chunk
            rows.append(row_base)

    for ln in lines:
        sec_here = current_section_of(ln)
        if sec_here:
            # cambia de secciÃ³n â†’ flush
            flush_buffer(sec, buf)
            sec = sec_here
            buf = []
        else:
            buf.append(ln)
    flush_buffer(sec, buf)
    return rows

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GOOGLE DRIVE (reemplazo por nombre)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_creds(scopes):
    sa = st.secrets.get("gcp_service_account")
    if not sa:
        return None
    if isinstance(sa, dict):
        return Credentials.from_service_account_info(sa, scopes=scopes)
    try:
        return Credentials.from_service_account_info(json.loads(sa), scopes=scopes)
    except Exception:
        return None

def drive_client(creds):
    return build("drive", "v3", credentials=creds)

def drive_find_file(drive, name: str, folder_id: str) -> str:
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = drive.files().list(q=q, fields="files(id,name)", pageSize=1).execute()
    files = res.get("files", [])
    return files[0]["id"] if files else ""

def drive_upload_replace(drive, folder_id: str, name: str, data: bytes, mime: str):
    file_id = drive_find_file(drive, name, folder_id)
    media = MediaIoBaseUpload(io.BytesIO(data), mimetype=mime, resumable=False)
    if file_id:
        # update contenido (mantiene el mismo id â†’ Looker sigue apuntando)
        drive.files().update(fileId=file_id, media_body=media).execute()
        return file_id
    else:
        meta = {"name": name, "parents": [folder_id]}
        f = drive.files().create(body=meta, media_body=media, fields="id").execute()
        return f["id"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("1) SubÃ­ el/los Ã“rdenes del DÃ­a (PDF o DOCX)")
uploads = st.file_uploader("ğŸ“‚ Archivos", type=["pdf","docx"], accept_multiple_files=True)

if not uploads:
    st.info("SubÃ­ al menos un archivo para continuar.")
    st.stop()

all_rows = []
for up in uploads:
    raw = extract_text_any(up)
    if not raw:
        st.warning(f"No se pudo leer: {up.name}")
        continue

    # AÃ±o / Fecha (metadatos base para cada Ã­tem)
    year, date_str = find_date_header(raw)
    base = {"aÃ±o": year, "fecha": date_str}
    lines = split_lines(raw)
    rows = parse_items_by_section(lines, base)
    # si el documento no trae ninguna secciÃ³n detectable, meterlo como "OTROS TEMAS"
    if not rows:
        r = empty_row(base)
        r["OTROS TEMAS"] = raw[:1500] + ("â€¦" if len(raw) > 1500 else "")
        rows = [r]
    all_rows.extend(rows)

if not all_rows:
    st.error("No se detectaron Ã­tems en los Ã“rdenes del DÃ­a cargados.")
    st.stop()

# ConstrucciÃ³n del DataFrame con columnas fijas (orden inmutable)
df = pd.DataFrame(all_rows)
# Asegurar todas las columnas (y el orden)
for col in FIXED_COLUMNS:
    if col not in df.columns:
        df[col] = ""
df = df[FIXED_COLUMNS]

st.success("âœ… Ã“rdenes del DÃ­a procesados.")
st.dataframe(df, use_container_width=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Descargas locales
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("2) Descargar planillas")
# CSV
csv_bytes = df.to_csv(index=False).encode("utf-8")
st.download_button("ğŸ“— CSV (OrdenDelDia_Consejo.csv)", data=csv_bytes, file_name=CSV_NAME, mime="text/csv")

# XLSX
def to_xlsx_bytes(df0: pd.DataFrame) -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as w:
        df0.to_excel(w, index=False, sheet_name=SHEET_NAME)
    buf.seek(0); return buf

xlsx_buf = to_xlsx_bytes(df)
st.download_button("ğŸ“˜ Excel (OrdenDelDia_Consejo.xlsx)", data=xlsx_buf, file_name=XLSX_NAME,
                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Subida a Google Drive (reemplazo)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("3) Subir/Reemplazar en Google Drive (para Looker Studio)")
folder_id = st.secrets.get("drive_folder_id", DEFAULT_FOLDER_ID)
creds = get_creds(["https://www.googleapis.com/auth/drive.file"])

if not creds:
    st.caption("â„¹ï¸ ConfigurÃ¡ `gcp_service_account` en Secrets para habilitar Drive.")
else:
    if st.button("ğŸš€ Subir/Reemplazar CSV y Excel en Drive"):
        try:
            drv = drive_client(creds)
            csv_id  = drive_upload_replace(drv, folder_id, CSV_NAME,  csv_bytes, "text/csv")
            xlsx_id = drive_upload_replace(drv, folder_id, XLSX_NAME, xlsx_buf.getvalue(),
                                           "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            st.success("âœ… Archivos actualizados en Drive con el mismo nombre (IDs preservados si ya existÃ­an).")
            st.caption(f"CSV id: {csv_id} Â· XLSX id: {xlsx_id}")
            st.info("Si tus fuentes de Looker Studio referencian estos archivos por ID o por nombre en esa carpeta, se verÃ¡n actualizadas automÃ¡ticamente.")
        except Exception as e:
            st.error(f"Error subiendo a Drive: {e}")
