# -*- coding: utf-8 -*-
import io, re, unicodedata
import pandas as pd
import streamlit as st

# Lectores de documentos
from pdfminer.high_level import extract_text as pdf_extract_text
from docx import Document as DocxDocument

# Google Sheets (opcional: solo si activ√°s el upload)
try:
    import gspread
    from google.oauth2.service_account import Credentials
    HAS_GS = True
except Exception:
    HAS_GS = False

st.set_page_config(page_title="Extractor de ACTAS ‚Üí Excel/Sheets", page_icon="üóÇÔ∏è", layout="centered")
st.title("üóÇÔ∏è Extractor de ACTAS del Consejo ‚Üí Excel y Google Sheets")

st.caption("Sub√≠ un PDF o DOCX de acta. La app detecta Proyectos, Informes (avance/final), Categorizaci√≥n, Jornadas, Cursos y trabajos para Revista Cuadernos.")

# -------------------- Config (patrones) --------------------
SECTION_DEFS = [
    {"name": "Proyectos de investigaci√≥n", "patterns": [r"Presentaci√≥n de Proyectos", r"Proyectos de Investigaci√≥n", r"Proyectos de Convocatoria Abierta"]},
    {"name": "Proyectos de c√°tedra",       "patterns": [r"Proyectos de Asignatura", r"Proyectos Cuadernos de C√°tedra"]},
    {"name": "Informes finales",            "patterns": [r"Presentaci√≥n de Informes Finales", r"Informes Finales"]},
    {"name": "Informes de avance",          "patterns": [r"Presentaci√≥n de Informes de Avance", r"Informes de Avance"]},
    {"name": "Categorizaci√≥n",              "patterns": [r"Solicitud de Categorizaci√≥n", r"Categorizaciones Extraordinarias", r"Categorizaci√≥n de investigadores", r"Categorizaci√≥n de docentes"]},
    {"name": "Jornadas de investigaci√≥n",   "patterns": [r"Jornadas de Investigaci√≥n", r"Jornadas internas de investigaci√≥n"]},
    {"name": "Trabajos Revista Cuadernos",  "patterns": [r"Revista Cuadernos", r"Cuadernos de la Secretar√≠a de Investigaci√≥n", r"presentaci√≥n de res√∫menes", r"trabajos para la revista", r"res√∫menes para Cuadernos"]},
    {"name": "Cursos",                      "patterns": [r"Cursos de capacitaci√≥n", r"Cursos"]},
]

FACULTY_HDR = re.compile(r"^(Facultad|Instituto Superior|Vicerrectorado|Escuela)\b.*", re.IGNORECASE)
ITEM_SPLIT = re.compile(r"\n\s*(?:[\u2022‚Ä¢\-]|[\u25CF\u25A0\u25E6]|\d+\.)\s*")

# -------------------- Utilidades --------------------
def _norm(s: str) -> str:
    if s is None:
        return ""
    s = s.replace("\x00", " ")
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\xa0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{2,}", "\n", s)
    return s.strip()

def read_pdf_bytes(file_bytes: bytes) -> str:
    # pdfminer requiere un path o un buffer; usamos BytesIO
    with io.BytesIO(file_bytes) as bio:
        text = pdf_extract_text(bio)
    return _norm(text or "")

def read_docx_bytes(file_bytes: bytes) -> str:
    with io.BytesIO(file_bytes) as bio:
        doc = DocxDocument(bio)
        text = "\n".join(p.text for p in doc.paragraphs)
    return _norm(text or "")

def find_acta_number(text: str) -> str:
    m = re.search(r"ACTA\s+N[¬∫¬∞]?\s*([0-9]+)", text, flags=re.IGNORECASE)
    return m.group(1).strip() if m else ""

def find_date_text(text: str) -> str:
    m = re.search(r"En la ciudad.*?\n(.*)", text, flags=re.IGNORECASE)
    if m: return _norm(m.group(1))
    m2 = re.search(r"a los\s+.+?d[i√≠]as.*?del mes de\s+.+?\s+de\s+dos mil.*", text, flags=re.IGNORECASE)
    return _norm(m2.group(0)) if m2 else ""

def split_sections(text: str):
    named = []
    for i, sec in enumerate(SECTION_DEFS):
        pat = "|".join([p for p in sec["patterns"]])
        named.append(f"(?P<s{i}>\\b(?:{pat})\\b)")
    if not named:
        return [("General", 0, len(text))]
    pattern = re.compile("|".join(named), flags=re.IGNORECASE)
    hits = []
    for m in pattern.finditer(text):
        for i, sec in enumerate(SECTION_DEFS):
            if m.group(f"s{i}"):
                hits.append((sec["name"], m.start()))
                break
    if not hits:
        return [("General", 0, len(text))]
    hits.sort(key=lambda x: x[1])
    spans = []
    for i, (name, start) in enumerate(hits):
        end = hits[i+1][1] if i+1 < len(hits) else len(text)
        spans.append((name, start, end))
    return spans

def chunk_by_faculty(section_text: str):
    lines = [ln.strip() for ln in section_text.split("\n") if ln.strip()]
    blocks, current, buf = [], None, []
    for ln in lines:
        if FACULTY_HDR.match(ln):
            if buf:
                blocks.append((current, "\n".join(buf)))
                buf = []
            current = ln
        else:
            buf.append(ln)
    if buf:
        blocks.append((current, "\n".join(buf)))
    return blocks or [(None, section_text)]

def extract_candidate_items(text: str):
    parts = ITEM_SPLIT.split("\n" + text)
    cands = []
    for p in parts:
        p = p.strip(" ;\n\t")
        if len(p) < 6: 
            continue
        if re.search(r"(Proyecto|Denominaci[o√≥]n|PROJOVI|Informe|Categorizaci[o√≥]n|Baja del proyecto|Revista|Cuadernos|Cursos?)", p, re.IGNORECASE):
            cands.append(p)
    return cands or [text.strip()]

def infer_estado(text: str) -> str:
    t = text.lower()
    if "baja del proyecto" in t or re.search(r"\bbaja\b", t):
        return "Baja"
    if "pr√≥rroga" in t or "prorroga" in t:
        return "Pr√≥rroga"
    if "aprob" in t and ("elev" in t or "enviado" in t):
        return "Aprobado y elevado"
    if "aprob" in t:
        return "Aprobado"
    if "solicitud de categorizaci√≥n" in t or "solicitud de categorizacion" in t:
        return "Solicitud"
    return ""

def infer_destino_publicacion(text: str) -> str:
    if re.search(r"\b(Cuadernos|revista\s+cuadernos|Cuadernos de la Secretar[i√≠]a de Investigaci[o√≥]n)\b", text, re.IGNORECASE):
        return "Revista Cuadernos"
    return ""

def extract_title_director(text: str):
    title, director = None, None
    m = re.search(r"Proyecto\s*:\s*(.+?)(?:\.\s*Director(?:a)?\s*:\s*([^.]+))?$", text, re.IGNORECASE)
    if m:
        title = m.group(1).strip(" .")
        if m.lastindex and m.lastindex >= 2 and m.group(2):
            director = m.group(2).strip(" .")
    if not director:
        m2 = re.search(r"Director(?:a)?\s*:\s*([^.]+)", text, re.IGNORECASE)
        if m2:
            director = m2.group(1).strip(" .")
    if not title:
        m3 = re.search(r"Denominaci[o√≥]n.*?:\s*(.+?)(?:\.|$)", text, re.IGNORECASE)
        if m3:
            title = m3.group(1).strip()
    if not title:
        m4 = re.search(r"PROJOVI\s*:\s*(.+?)(?:\.|$)", text, re.IGNORECASE)
        if m4:
            title = m4.group(1).strip()
    if not title:
        m5 = re.search(r"[¬´‚Äú\"']([^\"‚Äù¬ª']+)[\"‚Äù¬ª']", text)
        if m5:
            title = m5.group(1).strip()
    return title, director

def build_dataframe(text: str, source_name: str) -> pd.DataFrame:
    acta = find_acta_number(text)
    fecha = find_date_text(text)
    sections = split_sections(text)
    rows = []
    for sec_name, s, e in sections:
        chunk = text[s:e].strip()
        for faculty, block in chunk_by_faculty(chunk):
            for item in extract_candidate_items(block):
                title, director = extract_title_director(item)
                estado = infer_estado(block + " " + item)
                destino = infer_destino_publicacion(block + " " + item)
                rows.append({
                    "Acta": acta,
                    "Fecha": fecha,
                    "Facultad": faculty or "",
                    "Tipo_tema": sec_name,
                    "Titulo_o_denominacion": (title or item)[:400],
                    "Director": director or "",
                    "Estado": estado,
                    "Destino_publicacion": destino,
                    "Fuente_archivo": source_name,
                })
    df = pd.DataFrame(rows)
    if df.empty:
        return df
    for c in df.columns:
        df[c] = df[c].astype(str).str.strip()
    # Orden recomendado
    df = df[[
        "Acta","Fecha","Facultad","Tipo_tema","Titulo_o_denominacion",
        "Director","Estado","Destino_publicacion","Fuente_archivo"
    ]]
    return df

# -------------------- UI --------------------
file = st.file_uploader("Sub√≠ el acta (PDF o DOCX)", type=["pdf","docx"])
col1, col2 = st.columns(2)
with col1:
    sheet_name = st.text_input("Nombre del Google Sheet (opcional para subir)", value="Base Consejo de Investigaci√≥n")
with col2:
    ws_name = st.text_input("Nombre de la pesta√±a (worksheet)", value="Actas")

if file is not None:
    suffix = (file.name.split(".")[-1] or "").lower()
    raw = file.read()
    if suffix == "pdf":
        text = read_pdf_bytes(raw)
    elif suffix == "docx":
        text = read_docx_bytes(raw)
    else:
        text = ""
    if not text.strip():
        st.error("No se pudo leer el archivo.")
        st.stop()

    df = build_dataframe(text, file.name)
    if df.empty:
        st.warning("No se detectaron √≠tems. Revis√° el archivo o los encabezados.")
        st.stop()

    st.success("Extracci√≥n completada.")
    st.dataframe(df, use_container_width=True)

    # Descargar Excel
    st.subheader("Descargar Excel")
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="Actas")
    st.download_button(
        label="üíæ Descargar Excel (Actas.xlsx)",
        data=buf.getvalue(),
        file_name="Actas.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )

    # Subir a Google Sheets (opcional)
    st.subheader("Subir a Google Sheets (opcional)")
    st.caption("Requiere configurar credenciales en st.secrets['gcp_service_account'] (JSON).")
    can_upload = HAS_GS and ("gcp_service_account" in st.secrets)
    if not HAS_GS:
        st.info("Instal√° gspread y google-auth (requirements) para habilitar subida a Sheets.")
    elif "gcp_service_account" not in st.secrets:
        st.info("Agreg√° el secreto 'gcp_service_account' en Settings ‚Üí Secrets ‚Üí (contenido JSON de la Service Account).")
    do_upload = st.checkbox("Subir esta tabla a Google Sheets", value=False, disabled=(not can_upload))

    if do_upload and can_upload:
        try:
            scopes = ["https://www.googleapis.com/auth/spreadsheets"]
            creds = Credentials.from_service_account_info(st.secrets["gcp_service_account"], scopes=scopes)
            client = gspread.authorize(creds)
            try:
                sh = client.open(sheet_name)
            except gspread.exceptions.SpreadsheetNotFound:
                sh = client.create(sheet_name)
            try:
                ws = sh.worksheet(ws_name)
            except gspread.exceptions.WorksheetNotFound:
                ws = sh.add_worksheet(title=ws_name, rows=1000, cols=20)
            ws.clear()
            ws.update([df.columns.tolist()] + df.astype(str).values.tolist())
            st.success(f"‚úÖ Google Sheets actualizado: {sheet_name} / {ws_name} ({len(df)} filas)")
            st.caption("Conect√° esta hoja a Looker Studio y listo.")
        except Exception as e:
            st.error(f"Error subiendo a Google Sheets: {e}")
